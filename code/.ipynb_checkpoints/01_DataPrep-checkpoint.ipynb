{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2291d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5af12eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "raw = pd.read_csv('../data/raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53889c25",
   "metadata": {},
   "source": [
    "## basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c40772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the first and second rows so that the data has only one header row\n",
    "cleaned = raw.iloc[2:].copy()\n",
    "\n",
    "# Convert the 'StartDate' column to a datetime format\n",
    "cleaned['StartDate'] = pd.to_datetime(cleaned['StartDate'])\n",
    "\n",
    "# Filter the DataFrame to keep only the rows with dates on or after July 1, 2019\n",
    "cleaned = cleaned[cleaned['StartDate'] >= '2019-07-01']\n",
    "\n",
    "# Convert the 'Q51' column to a numeric data type\n",
    "cleaned['Q51'] = pd.to_numeric(cleaned['Q51'], errors='coerce')\n",
    "\n",
    "# Filter the DataFrame to remove rows where 'Q51' is greater than 30 or less than 17\n",
    "cleaned = cleaned[(cleaned['Q51'] <= 30) & (cleaned['Q51'] >= 17)].dropna(subset=['Q51'])\n",
    "                  \n",
    "# Filter the DataFrame to remove rows where '3', 'Q4', and 'Q5' are all 0 (has no relationship)\n",
    "cleaned = cleaned[~((cleaned['3'] == 0) & (cleaned['Q4'] == 0) & (cleaned['Q5'] == 0))]\n",
    "\n",
    "# Save the cleaned DataFrame to the 'data' folder\n",
    "cleaned.to_csv('../data/cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e2a3e",
   "metadata": {},
   "source": [
    "## add predictors that have only one item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af49275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned data \n",
    "cleaned = pd.read_csv('../data/cleaned_data.csv')\n",
    "\n",
    "# Create a new DataFrame called 'predictors' with some columns (no mean or dummy) from the cleaned data\n",
    "predictors = cleaned[['Q73', '1', '2', 'Q52', 'Q69', 'Q51','3', 'Q4', 'Q5', 'Q8',\n",
    "                      'Q7', 'Q9', 'Q63', 'Q37', 'Q71', 'Q41', 'Q63.1', 'Q76_6', 'Q20_1',\n",
    "                      'Q20_2', 'Q20_3', 'Q20_4', 'Q68_1',\n",
    "                                'Q68_2', 'Q68_3', 'Q68_4', 'Q69.1', 'Q21', 'Q79_1', 'Q79_4',\n",
    "                                'Q79_5', 'Q79_6', 'Q79_7', 'Q79_8', 'Q79_9', 'Q79_10', 'Q79_11',\n",
    "                                'Q79_12', 'Q79_13', 'Q35_1', 'Q35_2', 'Q35_4', 'Q35_3', 'Q35_5',\n",
    "                                'Q35_19', 'Q35_6', 'Q35_7', 'Q35_8', 'Q35_9', 'Q35_18', 'Q35_10',\n",
    "                                'Q35_12', 'Q35_17', 'Q35_13', 'Q35_11', 'Q35_16', 'Q36#2_1',\n",
    "                                'Q36#2_2', 'Q36#2_3']].copy()\n",
    "\n",
    "# Rename \n",
    "predictors = predictors.rename(columns= {'Q73': 'e1', '1': 'd1', '2': 'd2', 'Q52':\"d4\",\n",
    "                                        'Q69': 'd5', 'Q51': 'd6', '3': 'r1', 'Q4': 'r2',\n",
    "                                         'Q5': 'r3', 'Q8': 'r5', 'Q7': 'r6', 'Q9': 'r7',\n",
    "                                         'Q63': 'r8', 'Q37': 'r9', 'Q71': 'r10', 'Q41': 'r11',\n",
    "                                        'Q63.1': 'r13', 'Q76_6': 'r15', 'Q20_1': 'r19',\n",
    "                                        'Q20_2': 'r20', 'Q20_3': 'r21', 'Q20_4': 'r22',\n",
    "                                        'Q68_1': 'r24', 'Q68_2': 'r25', 'Q68_3': 'r26',\n",
    "                                        'Q68_4': 'r27', 'Q69.1': 'r28', 'Q21': 'f1',\n",
    "                                        'Q79_1': 'f3', 'Q79_4': 'f5', 'Q79_5': 'f6',\n",
    "                                        'Q79_6': 'f7', 'Q79_7': 'f8', 'Q79_8': 'f9',\n",
    "                                        'Q79_9': 'f10', 'Q79_10': 'f11', 'Q79_11': 'f12',\n",
    "                                        'Q79_12': 'f13', 'Q79_13': 'f14', 'Q35_1': 's1',\n",
    "                                        'Q35_2': 's2', 'Q35_4': 's3', 'Q35_3': 's4',\n",
    "                                        'Q35_5': 's5', 'Q35_19': 's6', 'Q35_6': 's7',\n",
    "                                        'Q35_7': 's8', 'Q35_8': 's9', 'Q35_9': 's10', 'Q35_18': 's11', 'Q35_10': 's12',\n",
    "                                        'Q35_12': 's13', 'Q35_17': 's14', 'Q35_13': 's15',\n",
    "                                        'Q35_11': 's16', 'Q35_16': 's17', 'Q36#2_1': 'f15',\n",
    "                                        'Q36#2_2': 'f16', 'Q36#2_3': 'f17'})\n",
    "                                       \n",
    "# Save \n",
    "predictors.to_csv('../data/predictors.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a07742c",
   "metadata": {},
   "source": [
    "\"\"\" \n",
    "After checking the new data, I found r1,r2,r3,r7,r8 need to be cleaned.\n",
    "I changed some values by hand: \n",
    "from 1200 (yes 10 years it's a long story) to 120),\n",
    "from 55+ to 55, from 36 (still ongoing) to 36,from married to empty,\n",
    "from 3-4 months to 4 months,from 2 1/2 years to 2.5 years,\n",
    "delete all \"N/A\", etc\n",
    "Then I use the following code to delete units (month/s or year/s) and create a seperate data\n",
    "note: I make this section as markdown to make sure it not be rerun when I need to restart & run all\n",
    "\"\"\" \n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean_duration_column(column):\n",
    "    \n",
    "    # Step 1: Convert non-numeric values to NaN\n",
    "    cleaned_column = pd.to_numeric(column, errors='coerce')\n",
    "\n",
    "    # Step 2: Identify durations expressed in years and convert them to months\n",
    "    years_pattern = re.compile(r'(\\d+([.,]?\\d+)?)\\s*years?')\n",
    "    cleaned_column = column.apply(lambda x: float(years_pattern.match(x).group(1)) * 12 if years_pattern.match(str(x)) else x)\n",
    "\n",
    "    # Step 3: Strip the unit 'months' from values that include it and handle the special case with additional text\n",
    "    months_pattern = re.compile(r'(\\d+)\\s*months?')\n",
    "    special_case_pattern = re.compile(r'(\\d+)\\s*\\(yes\\s*\\d+\\s*years')\n",
    "    cleaned_column = cleaned_column.apply(lambda x: int(months_pattern.match(str(x)).group(1)) if months_pattern.match(str(x)) else (int(special_case_pattern.match(str(x)).group(1)) if special_case_pattern.match(str(x)) else x))\n",
    "\n",
    "    # Step 4: Convert all values to integers\n",
    "    cleaned_column = cleaned_column.astype(float)\n",
    "\n",
    "    return cleaned_column\n",
    "\n",
    "predictors = pd.read_csv('../data/predictors.csv')\n",
    "\n",
    "# Clean the r3, r7, and r8 columns\n",
    "predictors['r3_cleaned'] = clean_duration_column(predictors['r3'])\n",
    "predictors['r7_cleaned'] = clean_duration_column(predictors['r7'])\n",
    "predictors['r8_cleaned'] = clean_duration_column(predictors['r8'])\n",
    "\n",
    "# Drop the original columns and rename the cleaned columns\n",
    "predictors.drop(columns=['r3', 'r7', 'r8'], inplace=True)\n",
    "predictors.rename(columns={'r3_cleaned': 'r3', 'r7_cleaned': 'r7', 'r8_cleaned': 'r8'}, inplace=True)\n",
    "\n",
    "# Save the cleaned predictors DataFrame to a new CSV file\n",
    "predictors.to_csv('../data/predictors_manu12378.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f117da43",
   "metadata": {},
   "source": [
    "## add predictors that are the means or the sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2027955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean for r12 variables\n",
    "r12_variables = ['Q12', 'Q11', 'Q13', 'Q14', 'Q15', 'Q16', 'Q17']\n",
    "reverse_code_r12 = ['Q14', 'Q17']\n",
    "\n",
    "for var in reverse_code_r12:\n",
    "    cleaned[var] = 6 - cleaned[var]\n",
    "\n",
    "cleaned['r12_mean'] = cleaned[r12_variables].mean(axis=1)\n",
    "\n",
    "# Calculate the mean for r14 variables\n",
    "r14_variables = ['Q76_1', 'Q76_2', 'Q76_3', 'Q76_4', 'Q76_5']\n",
    "cleaned['r14_mean'] = cleaned[r14_variables].mean(axis=1)\n",
    "\n",
    "# Calculate the mean for r16 variables\n",
    "r16_variables = ['Q19_1', 'Q19_2', 'Q19_3', 'Q19_4', 'Q19_5', 'Q19_6', 'Q19_7', 'Q19_8', 'Q19_9', 'Q19_10', 'Q19_11']\n",
    "reverse_code_r16 = ['Q19_2', 'Q19_7']\n",
    "\n",
    "for var in reverse_code_r16:\n",
    "    cleaned[var] = 8 - cleaned[var]\n",
    "\n",
    "cleaned['r16_mean'] = cleaned[r16_variables].mean(axis=1)\n",
    "\n",
    "# Calculate the mean for r18 variables\n",
    "r18_variables = ['Q18_1', 'Q18_2', 'Q18_3', 'Q18_4', 'Q18_5', 'Q18_6']\n",
    "reverse_code_r18 = ['Q18_1', 'Q18_3', 'Q18_5', 'Q18_6']\n",
    "\n",
    "for var in reverse_code_r18:\n",
    "    cleaned[var] = 8 - cleaned[var]\n",
    "\n",
    "cleaned['r18_mean'] = cleaned[r18_variables].mean(axis=1)\n",
    "\n",
    "# Define variable groups for calculating means\n",
    "f4_variables = ['Q79_2', 'Q79_3']\n",
    "f18_variables = ['Q50#1_1', 'Q50#1_3', 'Q50#1_4', 'Q50#1_6', 'Q50#1_9', 'Q50#1_10']\n",
    "f19_variables = ['Q50#1_2', 'Q50#1_5', 'Q50#1_8', 'Q50#1_12']\n",
    "f20_variables = ['Q50#1_7', 'Q50#1_11', 'Q50#1_13', 'Q50#1_14']\n",
    "f21_variables = ['Q50#2_1', 'Q50#2_3', 'Q50#2_4', 'Q50#2_6', 'Q50#2_9', 'Q50#2_10']\n",
    "f22_variables = ['Q50#2_2', 'Q50#2_5', 'Q50#2_8', 'Q50#2_12']\n",
    "f23_variables = ['Q50#2_7', 'Q50#2_11', 'Q50#2_13', 'Q50#2_14']\n",
    "f24_variables = ['Q77_1', 'Q77_2', 'Q77_3', 'Q77_4']\n",
    "\n",
    "\n",
    "# Calculate means for each variable group\n",
    "cleaned['f4_mean'] = cleaned[f4_variables].mean(axis=1)\n",
    "cleaned['f18_mean'] = cleaned[f18_variables].mean(axis=1)\n",
    "cleaned['f19_mean'] = cleaned[f19_variables].mean(axis=1)\n",
    "cleaned['f20_mean'] = cleaned[f20_variables].mean(axis=1)\n",
    "cleaned['f21_mean'] = cleaned[f21_variables].mean(axis=1)\n",
    "cleaned['f22_mean'] = cleaned[f22_variables].mean(axis=1)\n",
    "cleaned['f23_mean'] = cleaned[f23_variables].mean(axis=1)\n",
    "cleaned['f24_mean'] = cleaned[f24_variables].mean(axis=1)\n",
    "\n",
    "# Add the new columns to the predictors DataFrame\n",
    "predictors['r12'] = cleaned['r12_mean']\n",
    "predictors['r14'] = cleaned['r14_mean']\n",
    "predictors['r16'] = cleaned['r16_mean']\n",
    "predictors['r18'] = cleaned['r18_mean']\n",
    "predictors['f4'] = cleaned['f4_mean']\n",
    "predictors['f18'] = cleaned['f18_mean']\n",
    "predictors['f19'] = cleaned['f19_mean']\n",
    "predictors['f20'] = cleaned['f20_mean']\n",
    "predictors['f21'] = cleaned['f21_mean']\n",
    "predictors['f22'] = cleaned['f22_mean']\n",
    "predictors['f23'] = cleaned['f23_mean']\n",
    "predictors['f24'] = cleaned['f24_mean']\n",
    "\n",
    "# Save the updated predictors DataFrame to a CSV file\n",
    "predictors.to_csv('../data/predictors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf035657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned data \n",
    "cleaned = pd.read_csv('../data/cleaned_data.csv')\n",
    "\n",
    "# Create a temporary DataFrame with r17 columns\n",
    "r17_df = cleaned[['Q65_1', 'Q65_2', 'Q65_3', 'Q65_4', 'Q65_5', 'Q65_6', 'Q65_7', \n",
    "                  'Q65_8', 'Q65_9', 'Q65_10', 'Q65_11', 'Q65_12', 'Q65_13']].copy()\n",
    "    \n",
    "# Define the custom transformation function for normal items\n",
    "def normal_transformation(value):\n",
    "    if value == 1.0:\n",
    "        return 1\n",
    "    elif value == 2.0:\n",
    "        return -1\n",
    "    elif value in [3.0, 4.0]:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "    # Define the custom transformation function for reverse coding items\n",
    "def reverse_transformation(value):\n",
    "    if value == 1.0:\n",
    "        return -1\n",
    "    elif value == 2.0:\n",
    "        return 1\n",
    "    elif value in [3.0, 4.0]:\n",
    "        return -0.5\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "# Define columns for normal and reverse coding\n",
    "normal_code_columns = ['Q65_1', 'Q65_2', 'Q65_3', 'Q65_4', 'Q65_5', 'Q65_6', 'Q65_7', 'Q65_11', 'Q65_12']\n",
    "reverse_code_columns = ['Q65_8', 'Q65_9', 'Q65_10', 'Q65_13']\n",
    "\n",
    "# Apply the custom transformation to all r17 columns\n",
    "for col in normal_code_columns:\n",
    "    r17_df[col] = r17_df[col].apply(normal_transformation)\n",
    "\n",
    "for col in reverse_code_columns:\n",
    "    r17_df[col] = r17_df[col].apply(reverse_transformation)\n",
    "    \n",
    "# Custom function to set r17 to NaN if all 13 items are NaN\n",
    "def set_nan_if_all_nan(row):\n",
    "    if row.isna().all():\n",
    "        return np.nan\n",
    "    else:\n",
    "        return row.sum()\n",
    "\n",
    "# Apply the custom function to calculate r17\n",
    "predictors['r17'] = r17_df.apply(set_nan_if_all_nan, axis=1)\n",
    "\n",
    "# Save the updated predictors DataFrame to a CSV file\n",
    "predictors.to_csv('../data/predictors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fffb59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned data \n",
    "cleaned = pd.read_csv('../data/cleaned_data.csv')\n",
    "\n",
    "# Create a temporary DataFrame with f2 columns\n",
    "f2_df = cleaned[['Q22_1', 'Q22_2', 'Q22_3', 'Q22_4', 'Q22_5', 'Q22_6', 'Q22_7', \n",
    "                  'Q22_8', 'Q22_9', 'Q22_10', 'Q22_11', 'Q22_12', 'Q22_13']].copy()\n",
    "\n",
    "# Define the custom transformation function for normal items\n",
    "def f2_normal_transformation(value):\n",
    "    if value == 1.0:\n",
    "        return 1\n",
    "    elif value == 2.0:\n",
    "        return -1\n",
    "    elif value in [3.0, 4.0]:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Define the custom transformation function for reverse coding items\n",
    "def f2_reverse_transformation(value):\n",
    "    if value == 1.0:\n",
    "        return -1\n",
    "    elif value == 2.0:\n",
    "        return 1\n",
    "    elif value in [3.0, 4.0]:\n",
    "        return -0.5\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Define columns for normal and reverse coding\n",
    "f2_normal_code_columns = ['Q22_1', 'Q22_2', 'Q22_3', 'Q22_4', 'Q22_5', 'Q22_6', 'Q22_7', 'Q22_11', 'Q22_12']\n",
    "f2_reverse_code_columns = ['Q22_8', 'Q22_9', 'Q22_10', 'Q22_13']\n",
    "\n",
    "# Apply the custom transformation to all f2 columns\n",
    "for col in f2_normal_code_columns:\n",
    "    f2_df[col] = f2_df[col].apply(f2_normal_transformation)\n",
    "\n",
    "for col in f2_reverse_code_columns:\n",
    "    f2_df[col] = f2_df[col].apply(f2_reverse_transformation)\n",
    "\n",
    "# Apply the custom function to calculate f2\n",
    "predictors['f2'] = f2_df.apply(set_nan_if_all_nan, axis=1)\n",
    "\n",
    "# Save the updated predictors DataFrame to a CSV file\n",
    "predictors.to_csv('../data/predictors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702481ea",
   "metadata": {},
   "source": [
    "## dummy/polynomial code catigorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "17d31d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy code d1\n",
    "\n",
    "# Load the cleaned data \n",
    "predictors = pd.read_csv('../data/predictors.csv')\n",
    "\n",
    "# Create a new column 'd1_transformed' by combining the last 4 categories into 1\n",
    "predictors['d1_transformed'] = predictors['d1'].apply(lambda x: 3 if x in [3, 4, 5, 6] else x if x in [1, 2] else np.nan)\n",
    "\n",
    "# Create dummy variables for the 'd1_transformed' column\n",
    "dummies = pd.get_dummies(predictors['d1_transformed'], prefix='d1', drop_first=False)\n",
    "\n",
    "# Drop the last dummy variable\n",
    "dummies = dummies.iloc[:, :-1]\n",
    "\n",
    "# Concatenate the dummy variables to the predictors DataFrame\n",
    "final = pd.concat([predictors, dummies], axis=1)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final.to_csv('../data/final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6ff9bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild d2 \n",
    "# Replace the original values of d2 with the new values\n",
    "predictors['d2_transformed'] = predictors['d2'].apply(lambda x: 0 if x == 1 else 1 if x in [2, 3, 4, 5, 6, 7] else np.nan)\n",
    "\n",
    "# Add the transformed d2 column to the final DataFrame\n",
    "final['d2_transformed'] = predictors['d2_transformed']\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final.to_csv('../data/final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d572237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy code d3\n",
    "\n",
    "# Load the cleaned data\n",
    "cleaned_data = pd.read_csv('../data/cleaned_data.csv')\n",
    "\n",
    "# Define a function to assign the values for d3 based on Q54 values\n",
    "def assign_d3_values(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    \n",
    "    selected_options = set(map(int, value.split(',')))\n",
    "\n",
    "    if len(selected_options) == 1 and selected_options.issubset({1, 2, 3, 4}):\n",
    "        return selected_options.pop()\n",
    "    elif len(selected_options) == 1 and selected_options.issubset({6, 7}):\n",
    "        return 6\n",
    "    elif selected_options.intersection({5, 6}) or len(selected_options.intersection({1, 2, 3, 4})) > 1:\n",
    "        return 5\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to create the d3 variable\n",
    "cleaned_data['d3'] = cleaned_data['Q54'].apply(assign_d3_values)\n",
    "\n",
    "# Add the transformed d3 column to the final DataFrame\n",
    "final['d3'] = cleaned_data['d3']\n",
    "\n",
    "# Create dummy variables for the 'd3' column\n",
    "dummies_d3 = pd.get_dummies(cleaned_data['d3'], prefix='d3', drop_first=False)\n",
    "\n",
    "# Drop the last dummy variable\n",
    "dummies_d3 = dummies_d3.iloc[:, :-1]\n",
    "\n",
    "# Concatenate the dummy variables to the final DataFrame\n",
    "final = pd.concat([final, dummies_d3], axis=1)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final.to_csv('../data/final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7c9faf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy code d4\n",
    "# Transform d4 according to the new rules and save it as d4_transformed\n",
    "def new_d4_transformation(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    elif value == 1:\n",
    "        return 1\n",
    "    elif value == 2:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "predictors['d4_transformed'] = predictors['d4'].apply(new_d4_transformation)\n",
    "\n",
    "# Create dummy variables for the 'd4_transformed' column\n",
    "d4_dummies = pd.get_dummies(predictors['d4_transformed'], prefix='d4', drop_first=False)\n",
    "\n",
    "# Drop the last dummy variable\n",
    "d4_dummies = d4_dummies.iloc[:, :-1]\n",
    "\n",
    "# Concatenate the transformed d4 and its dummy variables to the final DataFrame\n",
    "final = pd.concat([final, predictors['d4_transformed'], d4_dummies], axis=1)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final.to_csv('../data/final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c23b7e",
   "metadata": {},
   "source": [
    "!pip install patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cca4f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "\n",
    "# polynomial code d5,r6 and r11\n",
    "\n",
    "# Load the predictors data\n",
    "predictors = pd.read_csv('../data/predictors.csv')\n",
    "# Load the final data\n",
    "final = pd.read_csv('../data/final.csv')\n",
    "\n",
    "# Create polynomial coding for the ordinal variable 'd5'\n",
    "d5_poly = dmatrix(\"C(d5, Poly)\", predictors, return_type='dataframe')\n",
    "\n",
    "# Drop the first column, which is the intercept\n",
    "d5_poly = d5_poly.iloc[:, 1:]\n",
    "\n",
    "# Rename the columns to have more descriptive names\n",
    "num_columns = d5_poly.shape[1]\n",
    "d5_poly.columns = [f'd5_poly_{i + 1}' for i in range(num_columns)]\n",
    "\n",
    "# Concatenate the polynomial coding columns to the final DataFrame\n",
    "final = pd.concat([final, d5_poly], axis=1)\n",
    "\n",
    "# Perform polynomial coding on the 'r6' variable\n",
    "r6_poly_code = dmatrix(\"C(r6, Poly)\", predictors, return_type=\"dataframe\")\n",
    "\n",
    "# Drop the intercept column\n",
    "r6_poly_code = r6_poly_code.iloc[:, 1:]\n",
    "\n",
    "# Rename the columns for the 'r6' polynomial coding\n",
    "num_columns_r6 = r6_poly_code.shape[1]\n",
    "r6_poly_code.columns = [f'r6_poly_{i + 1}' for i in range(num_columns_r6)]\n",
    "\n",
    "# Concatenate the polynomial coded variables to the predictors DataFrame\n",
    "final = pd.concat([final, r6_poly_code], axis=1)\n",
    "\n",
    "# Perform polynomial coding on the 'r11' variable\n",
    "r11_poly_code = dmatrix(\"C(r11, Poly)\", predictors, return_type=\"dataframe\")\n",
    "\n",
    "# Drop the intercept column\n",
    "r11_poly_code = r11_poly_code.iloc[:, 1:]\n",
    "\n",
    "# Rename the columns\n",
    "num_columns_r11 = r11_poly_code.shape[1]\n",
    "r11_poly_code.columns = [f\"r11_poly_{i + 1}\" for i in range(num_columns_r11)]\n",
    "\n",
    "# Concatenate the polynomial coded variables to the final DataFrame\n",
    "final = pd.concat([final, r11_poly_code], axis=1)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final.to_csv('../data/final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e037651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and transfor r4 to a binary variable\n",
    "# print(cleaned_data['Q6'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "80a94cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final data\n",
    "final = pd.read_csv('../data/final.csv')\n",
    "\n",
    "def relationship_transform(value):\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    else:\n",
    "        value_set = set(map(int, value.split(',')))\n",
    "        if value_set.intersection({2, 4, 5, 7}):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# Create 'r4' based on the 'Q6' column in cleaned_data\n",
    "predictors['r4'] = cleaned_data['Q6'].apply(relationship_transform)\n",
    "\n",
    "# Add 'r4' to the final DataFrame\n",
    "final['r4'] = predictors['r4']\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final.to_csv('../data/final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "04a2536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final data\n",
    "final = pd.read_csv('../data/final.csv')\n",
    "\n",
    "# Dummy code r28\n",
    "r28_dummies = pd.get_dummies(predictors['r28'], prefix='r28', drop_first=False)\n",
    "\n",
    "# Remove the last dummy variable\n",
    "r28_dummies = r28_dummies.iloc[:, :-1]\n",
    "\n",
    "# Rename the columns to have integers instead of floats\n",
    "r28_dummies.columns = [f'r28_{int(float(col.split(\"_\")[-1]))}' for col in r28_dummies.columns]\n",
    "\n",
    "# Concatenate the dummy variables to the final DataFrame\n",
    "final = pd.concat([final, r28_dummies], axis=1)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final.to_csv('../data/final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2da81b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final data\n",
    "final = pd.read_csv('../data/final.csv')\n",
    "\n",
    "def create_f25(row):\n",
    "    values = row[['Q58#1_1', 'Q58#1_2', 'Q58#1_3', 'Q58#1_4', 'Q58#1_5', 'Q58#1_6', 'Q58#1_7']].values\n",
    "    \n",
    "    if pd.isnull(values).all():\n",
    "        return np.nan\n",
    "    \n",
    "    if 1 in values[5:]: # Check if Q58#1_6 or Q58#1_7 is selected\n",
    "        return 1\n",
    "    elif 1 in values[:5]:  # Check if Q58#1_1, Q58#1_2, Q58#1_3, Q58#1_4, or Q58#1_5 is selected\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the cleaned_data DataFrame to create the f25 variable\n",
    "cleaned_data['f25'] = cleaned_data.apply(create_f25, axis=1)\n",
    "\n",
    "# Add f25 to the final DataFrame\n",
    "final['f25'] = cleaned_data['f25']\n",
    "\n",
    "def create_f26(row):\n",
    "    values = row[['Q58#2_1', 'Q58#2_2', 'Q58#2_3', 'Q58#2_4', 'Q58#2_5', 'Q58#2_6', 'Q58#2_7']].values\n",
    "    \n",
    "    if pd.isnull(values).all():\n",
    "        return np.nan\n",
    "    \n",
    "    if 1 in values[5:]: # Check if Q58#2_6 or Q58#2_7 is selected\n",
    "        return 1\n",
    "    elif 1 in values[:5]:  # Check if Q58#2_1, Q58#2_2, Q58#2_3, Q58#2_4, or Q58#2_5 is selected\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the cleaned_data DataFrame to create the f26 variable\n",
    "cleaned_data['f26'] = cleaned_data.apply(create_f26, axis=1)\n",
    "\n",
    "# Add f26 to the final DataFrame\n",
    "final['f26'] = cleaned_data['f26']\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "final.to_csv('../data/final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24ab19",
   "metadata": {},
   "source": [
    "## Create the data that would be used to conduct data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ce1bc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I would build a data called \"AApre\" to include all finalized variables. Since I have concern about\n",
    "the polynomial coding, I create \"AApre2\" that did not use polynomial coding, here I just used their \n",
    "original values (d5,r6,r11).\n",
    "1.There are 9 variables that need to be dummy code or polynomial code. Some of the \n",
    "original/transformed\n",
    "variables need to be romoved: d1,d2,d4,d5,r6,r11,d1_transformed,d2_transformed,d3,\n",
    "d4_transformed,r28.\n",
    "2.The variables that include manually cleaning (r1,r2,r3,r7,r8) would be added seperately from \n",
    "predictors_manu12378.csv.\n",
    "3.create the outcome variable r23: 1 for secure, 0 for insecure; delete cases that do not have \n",
    "values for r23\n",
    "4.delete cases that do not have values for f24\n",
    "5.scale the continuous variables in AApre\n",
    "6.after checking the missing values of the predictors, I decide to delete predictor r8 and all stress\n",
    "level predictors (s1-s17). Fill the rest missing values with mean\n",
    "\"\"\"\n",
    "# Load the final DataFrame \n",
    "final = pd.read_csv('../data/final.csv')\n",
    "\n",
    "# Remove the specified columns from the final DataFrame, but keep 'd2_transformed'\n",
    "final = final.drop(columns=['d1', 'd2', 'd4', 'd5', 'r6', 'r11', 'd1_transformed', 'd3', 'd4_transformed', 'r28'])\n",
    "\n",
    "# Load the cleaned_data and predictors_manu12378.csv files\n",
    "cleaned_data = pd.read_csv('../data/cleaned_data.csv')\n",
    "predictors_manu = pd.read_csv('../data/predictors_manu123578.csv')\n",
    "\n",
    "# Replace the original r1, r2, r3, r7, and r8 variables with the manually cleaned versions\n",
    "final['r1'] = predictors_manu['r1']\n",
    "final['r2'] = predictors_manu['r2']\n",
    "final['r3'] = predictors_manu['r3']\n",
    "final['r5'] = predictors_manu['r5']\n",
    "final['r7'] = predictors_manu['r7']\n",
    "final['r8'] = predictors_manu['r8']\n",
    "\n",
    "# Create the outcome variable 'r23' based on the 'Q68' variable in cleaned_data\n",
    "final['r23'] = cleaned_data['Q68'].apply(lambda x: 1 if x == 1 else 0 if x in [2, 3, 4] else np.nan)\n",
    "\n",
    "# Remove cases where 'f24' has no values\n",
    "final = final.dropna(subset=['r23'])\n",
    "\n",
    "# Remove cases where 'f24' has no values\n",
    "final = final.dropna(subset=['f24'])\n",
    "\n",
    "# Save the AApre DataFrame to a CSV file\n",
    "final.to_csv('../data/AApre.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e64333c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final DataFrame \n",
    "AApre = pd.read_csv('../data/AApre.csv')\n",
    "\n",
    "# Create a copy of the \"AApre\" DataFrame and call it \"AApre2\"\n",
    "AApre2 = AApre.copy()\n",
    "\n",
    "# Remove the polynomial coded variables from \"AApre2\"\n",
    "AApre2 = AApre2.drop(columns=['d5_poly_1', 'd5_poly_2', 'd5_poly_3', 'd5_poly_4', 'r6_poly_1', 'r6_poly_2', 'r6_poly_3', 'r11_poly_1', 'r11_poly_2', 'r11_poly_3', 'r11_poly_4'])\n",
    "\n",
    "# Add the original d5, r6, and r11 variables to \"AApre2\"\n",
    "AApre2[['d5', 'r6', 'r11']] = predictors[['d5', 'r6', 'r11']]\n",
    "\n",
    "# Save the AApre2 DataFrame to a CSV file\n",
    "AApre2.to_csv('../data/AApre2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c7333859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the continous varibales for AApre\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a list of continuous variable column names\n",
    "continuous_vars = ['e1', 'd6', 'r1', 'r2', 'r3', 'r5', 'r7', 'r8', 'r9', 'r10', 'r13', \n",
    "                   'r15', 'r19', 'r20', 'r21', 'r22', 'r24', 'r25', 'r26', 'r27', 'f1', \n",
    "                   'f3', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', \n",
    "                   's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', \n",
    "                   's12', 's13', 's14', 's15', 's16', 's17', 'f15', 'f16', 'f17', 'r12', \n",
    "                   'r14', 'r16', 'r18', 'f4', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', \n",
    "                   'f24', 'r17', 'f2']\n",
    "\n",
    "# Instantiate the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to your continuous variables\n",
    "scaler.fit(AApre[continuous_vars])\n",
    "\n",
    "# Transform the continuous variables\n",
    "AApre_scaled = AApre.copy()\n",
    "AApre_scaled[continuous_vars] = scaler.transform(AApre[continuous_vars])\n",
    "\n",
    "# Save the DataFrame with scaled variables to 'AApre.csv'\n",
    "AApre_scaled.to_csv('../data/AApre.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ce0d6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \n",
    "AApre = pd.read_csv('../data/AApre.csv')\n",
    "\n",
    "# Drop the specified columns\n",
    "AApre = AApre.drop(['r8'] + [f's{i}' for i in range(1, 18)], axis=1)\n",
    "\n",
    "# Fill the missing values with the mean\n",
    "AApre_filled = AApre.fillna(AApre.mean())\n",
    "\n",
    "# Save\n",
    "AApre_filled.to_csv('../data/AApre.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "947a709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the AApre2 dataset\n",
    "AApre2 = pd.read_csv(\"../data/AApre2.csv\")\n",
    "\n",
    "# Fill the missing values with the mean for each column\n",
    "AApre2_filled = AApre2.fillna(AApre2.mean())\n",
    "\n",
    "# Save\n",
    "AApre2_filled.to_csv('../data/AApre2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
