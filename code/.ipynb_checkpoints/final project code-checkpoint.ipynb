{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"final project code\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.875\n",
      "F1 Score: 0.8831168831168831\n",
      "Precision: 0.85\n",
      "Recall: 0.918918918918919\n",
      "Confusion Matrix:\n",
      " [[29  6]\n",
      " [ 3 34]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Load the data\n",
    "AApre = pd.read_csv(\"../data/AApre.csv\")\n",
    "\n",
    "# Convert the target variable to an integer type\n",
    "AApre['r23'] = AApre['r23'].astype(int)\n",
    "\n",
    "# Prepare the data for training and testing\n",
    "X = AApre.drop(['r23'], axis=1)\n",
    "y = AApre['r23']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the Logistic Regression model with ridge regularization\n",
    "log_reg_ridge = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "log_reg_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg_ridge.predict(X_test)\n",
    "\n",
    "# Compute the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Print the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most parameters in the earlier ridge model are default. I want to use GridSearchCV to conduct \n",
    "hyperparameter tuning. I tried several sets. However, the f1 score using the selected parameters is even samller than the previous model and sometimes it seems not converge. Therefore, I decide just use the first model I used (also the f1 and other indices are pretty high). I leave the tuning code here but turn them to be Raw NBConvert so that it will not be run."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = AApre.drop('r23', axis=1)\n",
    "y = AApre['r23']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameter grid for logistic regression\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # regularization parameter\n",
    "}\n",
    "\n",
    "# Perform cross-validation with GridSearchCV\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the model using the best hyperparameters\n",
    "best_logreg = grid_search.best_estimator_\n",
    "best_logreg.fit(X_train, y_train)\n",
    "score = best_logreg.score(X_test, y_test)\n",
    "print(\"F1: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients\n",
    "coefficients = log_reg_ridge.coef_\n",
    "\n",
    "# If you want to display the coefficients along with the corresponding predictor names, you can do the following:\n",
    "feature_names = X_train.columns\n",
    "coef_df = pd.DataFrame(coefficients.T, index=feature_names, columns=[\"Coefficient\"])\n",
    "\n",
    "# Save\n",
    "coef_df.to_csv('../data/log_coef.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the top 20 important predictors (based on the absolute value of the coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Coefficient\n",
      "70      r28_1     1.785356\n",
      "12        r20    -1.446066\n",
      "11        r19     1.390319\n",
      "74        f26     1.197516\n",
      "10        r15    -1.137841\n",
      "14        r22    -1.044619\n",
      "34        r12     0.893660\n",
      "58  d5_poly_1    -0.823247\n",
      "4          r3     0.797089\n",
      "0          e1    -0.767862\n",
      "69         r4    -0.713953\n",
      "41        f20     0.704620\n",
      "49       d1_2     0.664277\n",
      "48       d1_1    -0.597244\n",
      "60  d5_poly_3    -0.589129\n",
      "54       d3_4    -0.566714\n",
      "27        f11     0.527881\n",
      "55       d3_5    -0.525399\n",
      "30        f14    -0.475406\n",
      "52       d3_2     0.465625\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "coef_df = pd.read_csv(\"../data/log_coef.csv\")\n",
    "\n",
    "# Sort the coef_df DataFrame by the absolute value of the coefficients in descending order\n",
    "sorted_coef_df = coef_df.reindex(coef_df.Coefficient.abs().sort_values(ascending=False).index)\n",
    "\n",
    "# Get the top 20 important predictors\n",
    "top_20_predictors = sorted_coef_df.head(20)\n",
    "\n",
    "# Print the top 20 important predictors with their coefficients\n",
    "print(top_20_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1: Rank 1\n",
      "r3: Rank 1\n",
      "r15: Rank 1\n",
      "r19: Rank 1\n",
      "r20: Rank 1\n",
      "r21: Rank 1\n",
      "r22: Rank 1\n",
      "r26: Rank 1\n",
      "r12: Rank 1\n",
      "f20: Rank 1\n",
      "r17: Rank 1\n",
      "d1_1: Rank 1\n",
      "d1_2: Rank 1\n",
      "d2_transformed: Rank 1\n",
      "d3_4: Rank 1\n",
      "d3_5: Rank 1\n",
      "d5_poly_1: Rank 1\n",
      "r4: Rank 1\n",
      "r28_1: Rank 1\n",
      "f26: Rank 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Create a ridge logistic regression model\n",
    "ridge_model = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "\n",
    "# Create an RFE instance with the ridge model and desired number of features\n",
    "num_features = 20\n",
    "rfe = RFE(estimator=ridge_model, n_features_to_select=num_features)\n",
    "\n",
    "# Fit RFE on the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the ranking of the features\n",
    "feature_ranking = rfe.ranking_\n",
    "\n",
    "# Print the top 20 features\n",
    "top_features = sorted(zip(feature_ranking, X.columns), key=lambda x: x[0])[:num_features]\n",
    "for rank, feature in top_features:\n",
    "    print(f\"{feature}: Rank {rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r28_1: Coefficient 1.7853555496069964\n",
      "r20: Coefficient -1.4460660782501642\n",
      "r19: Coefficient 1.3903191437694333\n",
      "f26: Coefficient 1.1975158071645657\n",
      "r15: Coefficient -1.1378413327197354\n",
      "r22: Coefficient -1.0446190042587669\n",
      "r12: Coefficient 0.8936598810472284\n",
      "d5_poly_1: Coefficient -0.8232473553776275\n",
      "r3: Coefficient 0.7970892440663342\n",
      "e1: Coefficient -0.7678622546481553\n",
      "r4: Coefficient -0.7139530876741423\n",
      "f20: Coefficient 0.704620311297995\n",
      "d1_2: Coefficient 0.6642774059036025\n",
      "d1_1: Coefficient -0.5972444550068358\n",
      "d5_poly_3: Coefficient -0.5891285004726956\n",
      "d3_4: Coefficient -0.5667143925714715\n",
      "f11: Coefficient 0.5278813522334901\n",
      "d3_5: Coefficient -0.5253986126348164\n",
      "f14: Coefficient -0.4754057195608457\n",
      "d3_2: Coefficient 0.46562510449084077\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients from the fitted model\n",
    "coefficients = log_reg_ridge.coef_[0]\n",
    "\n",
    "# Create a dictionary with the feature names and their coefficients\n",
    "feature_coefficients = dict(zip(X_train.columns, coefficients))\n",
    "\n",
    "# Sort the dictionary by the absolute value of coefficients in descending order\n",
    "sorted_features = sorted(feature_coefficients.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print the top 20 features with their coefficients\n",
    "for feature, coef in sorted_features[:20]:\n",
    "    print(f\"{feature}: Coefficient {coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r28_1: Coefficient 1.7853555496069964\n",
      "r20: Coefficient -1.4460660782501642\n",
      "r19: Coefficient 1.3903191437694333\n",
      "f26: Coefficient 1.1975158071645657\n",
      "r15: Coefficient -1.1378413327197354\n",
      "r22: Coefficient -1.0446190042587669\n",
      "r12: Coefficient 0.8936598810472284\n",
      "d5_poly_1: Coefficient -0.8232473553776275\n",
      "r3: Coefficient 0.7970892440663342\n",
      "e1: Coefficient -0.7678622546481553\n",
      "r4: Coefficient -0.7139530876741423\n",
      "f20: Coefficient 0.704620311297995\n",
      "d1_2: Coefficient 0.6642774059036025\n",
      "d1_1: Coefficient -0.5972444550068358\n",
      "d3_4: Coefficient -0.5667143925714715\n",
      "d3_5: Coefficient -0.5253986126348164\n",
      "d2_transformed: Coefficient -0.4614299229362605\n",
      "r26: Coefficient 0.4555897862969526\n",
      "r21: Coefficient -0.42514430397013975\n",
      "r17: Coefficient 0.2351557121447659\n"
     ]
    }
   ],
   "source": [
    "# Get the RFE selected features and their coefficients\n",
    "rfe_selected_features = X_train.columns[rfe.support_]\n",
    "rfe_selected_coef = log_reg_ridge.coef_[0][rfe.support_]\n",
    "\n",
    "# Sort the RFE selected features by the absolute value of coefficients in descending order\n",
    "sorted_rfe_selected = sorted(zip(rfe_selected_features, rfe_selected_coef), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Print the top 20 features with their coefficients\n",
    "for feature, coef in sorted_rfe_selected:\n",
    "    print(f\"{feature}: Coefficient {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is less sensitive to scale and does not require polynomial code for ordinal variable, so I tried to use AApre2 data to conduct the analysis. Then I decided to also try AApre data and found it helped to improve the f1 and other indices. Therefore, I decided to use all anylysis based on AApre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8611111111111112\n",
      "F1 Score: 0.8831168831168831\n",
      "Precision: 0.85\n",
      "Recall: 0.918918918918919\n"
     ]
    }
   ],
   "source": [
    "# use the AApre2 dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the AApre2 dataset\n",
    "AApre2 = pd.read_csv(\"../data/AApre2.csv\")\n",
    "\n",
    "# Separate predictors and the target variable\n",
    "X = AApre2.drop('r23', axis=1)\n",
    "y = AApre2['r23']\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the random forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy and confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9027777777777778\n",
      "F1 Score: 0.8831168831168831\n",
      "Precision: 0.85\n",
      "Recall: 0.918918918918919\n"
     ]
    }
   ],
   "source": [
    "# use the AApre dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the AApre2 dataset\n",
    "AApre = pd.read_csv(\"../data/AApre.csv\")\n",
    "\n",
    "# Separate predictors and the target variable\n",
    "X = AApre.drop('r23', axis=1)\n",
    "y = AApre['r23']\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the random forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy and confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, try to use GridSearchCV to conduct hyperparameter tuning (tried different sets). However, similar things happened. Therefore, I decide just use the first model I used (also the f1 and other indices are pretty high). I leave the tuning code here but turn them to be Raw NBConvert so that it will not be run."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load the AApre2 dataset\n",
    "AApre = pd.read_csv(\"../data/AApre.csv\")\n",
    "\n",
    "# Separate predictors and the target variable\n",
    "X = AApre.drop('r23', axis=1)\n",
    "y = AApre['r23']\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 1000],\n",
    "}\n",
    "\n",
    "# Create a RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the model using GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy and confusion matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n {conf_mat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 important features:\n",
      "r19: 0.11873145124907396\n",
      "r20: 0.09690840954147496\n",
      "r12: 0.054647059041433355\n",
      "r16: 0.04123295432521759\n",
      "r22: 0.04016608547257021\n",
      "r28_1: 0.03939101286217848\n",
      "r14: 0.032330391117104215\n",
      "r17: 0.031424058048790254\n",
      "r18: 0.0313162692470644\n",
      "r21: 0.027319154597422183\n",
      "r3: 0.025547171166636448\n",
      "r7: 0.020233659841489\n",
      "f24: 0.014908389564289685\n",
      "r25: 0.014879065066155012\n",
      "r9: 0.014666596225914309\n",
      "f21: 0.014441985080063066\n",
      "r13: 0.014348789809781387\n",
      "r5: 0.01368737594115344\n",
      "f18: 0.013218032715476278\n",
      "f23: 0.013102237651855492\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fit the Random Forest model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Get the indices of the top 20 most important features\n",
    "indices = np.argsort(importances)[-20:][::-1]\n",
    "\n",
    "# Print the top 20 most important features and their importances\n",
    "print(\"Top 20 important features:\")\n",
    "\n",
    "for index, importance in zip(indices, importances[indices]):\n",
    "    print(f\"{AApre.columns[index]}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 features using RFE with Random Forest:\n",
      " Index(['r3', 'r7', 'r13', 'r19', 'r20', 'r21', 'r22', 'r26', 'r12', 'r14',\n",
      "       'r16', 'r18', 'f18', 'f20', 'f21', 'f22', 'f24', 'r17', 'f2', 'r28_1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Using RFE with Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rfe = RFE(estimator=rf, n_features_to_select=20)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "selected_features = rfe.support_\n",
    "feature_ranking = rfe.ranking_\n",
    "\n",
    "top_features = X.columns[selected_features]\n",
    "print(\"\\nTop 20 features using RFE with Random Forest:\\n\", top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 important features using RFE with Random Forest:\n",
      "r19: 0.14435515142026106\n",
      "r20: 0.13282160936240675\n",
      "r12: 0.08217676890723562\n",
      "r28_1: 0.07210267015563802\n",
      "r16: 0.056865932819892905\n",
      "r18: 0.05658040054489748\n",
      "r17: 0.05456716410583319\n",
      "r22: 0.04978787591288969\n",
      "r14: 0.041709973453233386\n",
      "r3: 0.036652407613525416\n",
      "r21: 0.036147631322720496\n",
      "r7: 0.03146504228899231\n",
      "f21: 0.02920571017079186\n",
      "f24: 0.02880164937616581\n",
      "f22: 0.025985367585507155\n",
      "f20: 0.025107846685218697\n",
      "f18: 0.024982305044278647\n",
      "f2: 0.024928554304853752\n",
      "r13: 0.023464369231641143\n",
      "r26: 0.02229156969401651\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest model on the selected features\n",
    "X_selected = X[top_features]\n",
    "X_train_selected, X_test_selected, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "rf_selected = RandomForestClassifier(random_state=42)\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get feature importances for the selected features\n",
    "importances_selected = rf_selected.feature_importances_\n",
    "\n",
    "# Get the indices of the top 20 most important features (in descending order)\n",
    "indices_selected = np.argsort(importances_selected)[::-1]\n",
    "\n",
    "# Print the top 20 most important features and their importances\n",
    "print(\"Top 20 important features using RFE with Random Forest:\")\n",
    "\n",
    "for index, importance in zip(indices_selected, importances_selected[indices_selected]):\n",
    "    print(f\"{top_features[index]}: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use these four sets of predictors to refit their own models respectively and check the f1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_f1_score(predictors, X, y):\n",
    "    X_selected = X[predictors]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Load the AApre dataset\n",
    "AApre = pd.read_csv(\"../data/AApre.csv\")\n",
    "\n",
    "# Separate predictors and the target variable\n",
    "X_AApre = AApre.drop('r23', axis=1)\n",
    "y_AApre = AApre['r23']\n",
    "\n",
    "# Define the predictors for each model\n",
    "ridge_top = ['r28_1', 'r20', 'r19', 'f26', 'r15', 'r22', 'r12', 'd5_poly_1', 'r3', 'e1', 'r4', 'f20', 'd1_2', 'd1_1', 'd5_poly_3', 'd3_4', 'f11', 'd3_5', 'f14', 'd3_2']\n",
    "ridge_rfe_top = ['r28_1', 'r20', 'r19', 'f26', 'r15', 'r22', 'r12', 'd5_poly_1', 'r3', 'e1', 'r4', 'f20', 'd1_2', 'd1_1', 'd3_4', 'd3_5', 'f11', 'd2_transformed', 'r26', 'r17']\n",
    "random_forest_top = ['r19', 'r20', 'r12', 'r16', 'r22', 'r28_1', 'r14', 'r17', 'r18', 'r21', 'r3', 'r7', 'f24', 'r25', 'r9', 'f21', 'r13', 'r5', 'f18', 'f23']\n",
    "random_forest_rfe_top = ['r19', 'r20', 'r12', 'r28_1', 'r16', 'r18', 'r17', 'r22', 'r14', 'r3', 'r21', 'r7', 'f21', 'f24', 'f22', 'f20', 'f18', 'f2', 'r13', 'r26']\n",
    "\n",
    "# Compute F1 scores for each set of predictors\n",
    "ridge_f1 = compute_f1_score(ridge_top, X_AApre, y_AApre)\n",
    "ridge_rfe_f1 = compute_f1_score(ridge_rfe_top, X_AApre, y_AApre)\n",
    "random_forest_f1 = compute_f1_score(random_forest_top, X_AApre, y_AApre)\n",
    "random_forest_rfe_f1 = compute_f1_score(random_forest_rfe_top, X_AApre, y_AApre)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score for Ridge predictors:\", ridge_f1)\n",
    "print(\"F1 Score for Ridge & RFE predictors:\", ridge_rfe_f1)\n",
    "print(\"F1 Score for Random Forest predictors:\", random_forest_f1)\n",
    "print(\"F1 Score for Random Forest & RFE predictors:\", random_forest_rfe_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the key predictors to refit their own models respectively and check the f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Ridge predictors: 0.7733333333333334\n",
      "F1 Score for Ridge & RFE predictors: 0.7733333333333334\n",
      "F1 Score for Random Forest predictors: 0.7631578947368421\n",
      "F1 Score for Random Forest & RFE predictors: 0.7631578947368421\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_f1_score(predictors, X, y):\n",
    "    X_selected = X[predictors]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Load the AApre dataset\n",
    "AApre = pd.read_csv(\"../data/AApre.csv\")\n",
    "\n",
    "# Separate predictors and the target variable\n",
    "X_AApre = AApre.drop('r23', axis=1)\n",
    "y_AApre = AApre['r23']\n",
    "\n",
    "# Define the predictors for each model\n",
    "ridge_top2 = ['r28_1','f26', 'r15', 'r12', 'd5_poly_1', 'r3']\n",
    "ridge_rfe_top2 = ['r28_1', 'f26', 'r15', 'r12', 'd5_poly_1', 'r3']\n",
    "random_forest_top2 = ['r12', 'r16', 'r28_1', 'r17', 'r18', 'r3']\n",
    "random_forest_rfe_top2 = ['r12', 'r28_1', 'r16', 'r18', 'r17', 'r3']\n",
    "\n",
    "# Compute F1 scores for each set of predictors\n",
    "ridge_f1k = compute_f1_score(ridge_top2, X_AApre, y_AApre)\n",
    "ridge_rfe_f1k = compute_f1_score(ridge_rfe_top2, X_AApre, y_AApre)\n",
    "random_forest_f1k = compute_f1_score(random_forest_top2, X_AApre, y_AApre)\n",
    "random_forest_rfe_f1k = compute_f1_score(random_forest_rfe_top2, X_AApre, y_AApre)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"F1 Score for Ridge predictors:\", ridge_f1k)\n",
    "print(\"F1 Score for Ridge & RFE predictors:\", ridge_rfe_f1k)\n",
    "print(\"F1 Score for Random Forest predictors:\", random_forest_f1k)\n",
    "print(\"F1 Score for Random Forest & RFE predictors:\", random_forest_rfe_f1k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
